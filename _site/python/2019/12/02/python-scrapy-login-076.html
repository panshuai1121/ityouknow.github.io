<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- Favicon Icon -->
    <link rel="shortcut icon" type="image/x-icon" href="/assets/images/favicon.ico">

    <title> 第76天：Scrapy 模拟登陆 - 潘帅的博客 </title>
    <meta name="keywords" content="ityouknow,Spring,Spring Boot,Spring Cloud,MongoDB,Jvm,Docker,生活故事,架构,大数据,一线,FastDFS,开发者,编程,代码,开源,IT网站,Developer,Programmer,Coder,Geek,IT技术博客,Java,Python,">
    <meta name="description"
          content="&lt;p&gt;by 闲欢&lt;/p&gt;
">

    <link rel="canonical" href="https://panshuai1121.github.io/python/2019/12/02/python-scrapy-login-076.html">
    <link rel="alternate" type="application/rss+xml" title="潘帅的博客" href="https://panshuai1121.github.io/feed.xml">

    <!-- Third-Party CSS -->
    <link rel="stylesheet" href="http://favorites.ren/bower_components/bootstrap/dist/css/bootstrap.min.css?v=1">
    <link rel="stylesheet" href="http://panshuai1121.github.io/bower_components/octicons/octicons/octicons.css?v=1">
    <link rel="stylesheet" href="http://favorites.ren/bower_components/hover/css/hover-min.css">
    <link rel="stylesheet" href="http://favorites.ren/bower_components/primer-markdown/dist/user-content.min.css">
    <link rel="stylesheet" href="http://favorites.ren/assets/css/syntax.css">
    <link rel="stylesheet" href="http://favorites.ren/assets/css/gitalk.css">

    

    <!-- My CSS -->
    <link rel="stylesheet" href="http://favorites.ren/assets/css/common.css">

    <!-- CSS set in page -->
    

    <!-- CSS set in layout -->
    
    <link rel="stylesheet" href="http://favorites.ren/assets/css/sidebar-post-nav.css">
    

    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css">

    <script type="text/javascript" src="http://favorites.ren/bower_components/jquery/dist/jquery.min.js"></script>
    <script type="text/javascript" src="http://favorites.ren/bower_components/bootstrap/dist/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="http://panshuai1121.github.io/assets/js/lock.js?v=1"></script>
</head>


    <body>

    <header class="site-header">
    <div class="site-header-topbar">
        <div class="container">
            <div class="topbar-menu">
                
                <div class="item">
                    <a href="/it.html"
                       target="_self"
                       title="深度">
                        深度
                    </a>
                </div>
                
                <div class="item">
                    <a href="/life.html"
                       target="_self"
                       title="故事">
                        故事
                    </a>
                </div>
                
                <div class="item">
                    <a href="/arch.html"
                       target="_self"
                       title="架构">
                        架构
                    </a>
                </div>
                
                <div class="item">
                    <a href="/link.html"
                       target="_self"
                       title="友链">
                        友链
                    </a>
                </div>
                
            </div>
        </div>
    </div>
    <div class="container">
        <a id="site-header-brand" href="/" title="潘帅的博客">
            <span class="octicon octicon-smiley"></span>
            潘帅的博客
        </a>
        <nav class="site-header-nav" role="navigation">
            
            <div class=" site-header-nav-item hvr-underline-from-center">
                <a href="/"
                   target=""
                   title="Home">
                    Home
                </a>
                
            </div>
            
            <div class=" site-header-nav-item hvr-underline-from-center">
                <a href="/spring-boot.html"
                   target="_self"
                   title="Spring Boot">
                    Spring Boot
                </a>
                
                <ul class="submenu">
                    
                    <li><a href="/spring-boot.html" 
                           target=""
                     >Spring Boot</a></li>
                    
                    <li><a href="/spring-cloud.html" 
                           target=""
                     >Spring Cloud</a></li>
                    
                </ul>
                
            </div>
            
            <div class=" site-header-nav-item hvr-underline-from-center">
                <a href="/php.html"
                   target="_self"
                   title="PHP">
                    PHP
                </a>
                
            </div>
            
            <div class=" site-header-nav-item hvr-underline-from-center">
                <a href="/python.html"
                   target=""
                   title="Python">
                    Python
                </a>
                
                <ul class="submenu">
                    
                    <li><a href="/python.html" 
                           target=""
                     >Python 教程</a></li>
                    
                    <li><a href="http://www.justdopython.com" 
                           target="_blank"
                     >Python 技术</a></li>
                    
                </ul>
                
            </div>
            
            <div class=" site-header-nav-item hvr-underline-from-center">
                <a href="/archives.html"
                   target="_self"
                   title="Archives">
                    Archives
                </a>
                
            </div>
            
        </nav>
    </div>
</header>


        <div class="content">
            <section class="jumbotron geopattern" data-pattern-id="第76天：Scrapy 模拟登陆">
    <div class="container">
        <div id="jumbotron-meta-info">        
            <h1>第76天：Scrapy 模拟登陆</h1>
            <span class="meta-info">
                
                
                <span class="octicon octicon-calendar"></span> 2019/12/02
                
            </span>
        </div>
    </div>
</section>
<script>
    $(document).ready(function(){

        $('.geopattern').each(function(){
            $(this).geopattern($(this).data('pattern-id'));
        });

    });
</script>
<article class="post container " itemscope itemtype="http://schema.org/BlogPosting">

    <div class="row">

        
        <div class="col-md-9 markdown-body">

            <p>by 闲欢</p>

<p>想爬取网站数据？先登录网站！对于大多数大型网站来说，想要爬取他们的数据，第一道门槛就是登录网站。下面请跟随我的步伐来学习如何模拟登陆网站。
<!--more--></p>

<h2 id="为什么进行模拟登陆">为什么进行模拟登陆？</h2>

<p>互联网上的网站分两种：需要登录和不需要登录。（这是一句废话！）</p>

<p>那么，对于不需要登录的网站，我们直接获取数据即可，简单省事。而对于需要登录才可以查看数据或者不登录只能查看一部分数据的网站来说，我们只好乖乖地登录网站了。（除非你直接黑进人家数据库，黑客操作请慎用！）</p>

<p>所以，对于需要登录的网站，我们需要模拟一下登录，一方面为了获取登陆之后页面的信息和数据，另一方面为了拿到登录之后的 cookie ，以便下次请求时使用。</p>

<h2 id="模拟登陆的思路">模拟登陆的思路</h2>

<p>一提到模拟登陆，大家的第一反应肯定是：切！那还不简单？打开浏览器，输入网址，找到用户名密码框，输入用户名和密码，然后点击登陆就完事！</p>

<p>这种方式没毛病，我们的 selenium 模拟登陆就是这么操作的。</p>

<p>除此之外呢，我们的 Requests 还可以直接携带已经登陆过的 cookies 进行请求，相当于绕过了登陆。</p>

<p>我们也可以利用 Requests 发送 post 请求，将网站登录需要的信息附带到 post 请求中进行登录。</p>

<p>以上就是我们常见的三种模拟登陆网站的思路，那么我们的 Scrapy 也使用了后两种方式，毕竟第一种只是 selenium 特有的方式。</p>

<p>Scrapy 模拟登陆的思路：</p>

<blockquote>
  <p>1、直接携带已经登陆过的 cookies 进行请求<br />
2、将网站登录需要的信息附带到 post 请求中进行登录</p>
</blockquote>

<h2 id="模拟登陆实例">模拟登陆实例</h2>

<h3 id="携带-cookies-模拟登陆">携带 cookies 模拟登陆</h3>
<p>每种登陆方式都有它的优缺点以及使用场景，我们来看看携带 cookies 登陆的应用场景：</p>

<blockquote>
  <p>1、cookie 过期时间很长，我们可以登录一次之后不用担心登录过期问题，常见于一些不规范的网站。<br />
2、我们能在 cookie 过期之前把我们需要的所有数据拿到。<br />
3、我们可以配合其他程序使用，比如使用 selenium 把登录之后的 cookie 获取保存到本地，然后在 Scrapy 发送请求之前先读取本地 cookie 。</p>
</blockquote>

<p>下面我们通过模拟登录被我们遗忘已久的人人网来讲述这种模拟登陆方式。</p>

<p>我们首先创建一个 Scrapy 项目：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; scrapy startproject login
</code></pre></div></div>

<p>为了爬取顺利，请先将 settings 里面的 robots 协议设置为 False ：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ROBOTSTXT_OBEY = False
</code></pre></div></div>

<p>接着，我们创建一个爬虫：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; scrapy genspider renren renren.com
</code></pre></div></div>
<p>我们打开 spiders 目录下的 renren.py ，代码如下：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># -*- coding: utf-8 -*-
import scrapy


class RenrenSpider(scrapy.Spider):
    name = 'renren'
    allowed_domains = ['renren.com']
    start_urls = ['http://renren.com/']

    def parse(self, response):
        pass

</code></pre></div></div>

<p>我们知道，<code class="highlighter-rouge">start_urls</code> 存的是我们需要爬取的第一个网页地址，这是我们爬数据的初始网页，假设我需要爬取人人网的个人中心页的数据，那么我登录人人网后，进入到个人中心页，网址是：<code class="highlighter-rouge">http://www.renren.com/972990680/profile</code> ，如果我直接将这个网址放到 <code class="highlighter-rouge">start_urls</code>  里面，然后我们直接请求，大家想一下，可不可以成功？</p>

<p>不可以，对吧！因为我们还没有登录，根本看不到个人中心页。</p>

<p>那么我们的登录代码加到哪里呢？</p>

<p>我们能确定的是我们必须在框架请求 <code class="highlighter-rouge">start_urls</code> 中的网页之前登录。</p>

<p>我们进入 Spider 类的源码，找到下面这一段代码：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def start_requests(self):
        cls = self.__class__
        if method_is_overridden(cls, Spider, 'make_requests_from_url'):
            warnings.warn(
                "Spider.make_requests_from_url method is deprecated; it "
                "won't be called in future Scrapy releases. Please "
                "override Spider.start_requests method instead (see %s.%s)." % (
                    cls.__module__, cls.__name__
                ),
            )
            for url in self.start_urls:
                yield self.make_requests_from_url(url)
        else:
            for url in self.start_urls:
                yield Request(url, dont_filter=True)

    def make_requests_from_url(self, url):
        """ This method is deprecated. """
        return Request(url, dont_filter=True)

</code></pre></div></div>

<p>我们从这段源码中可以看到，这个方法从 <code class="highlighter-rouge">start_urls</code> 中获取 URL ，然后构造一个 Request 对象来请求。既然这样，我们就可以重写 <code class="highlighter-rouge">start_requests</code> 方法来做一些事情，也就是在构造 <code class="highlighter-rouge">Request</code> 对象的时候把 cookies 信息加进去。</p>

<p>重写之后的 <code class="highlighter-rouge">start_requests</code> 方法如下：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># -*- coding: utf-8 -*-
import scrapy
import re

class RenrenSpider(scrapy.Spider):
    name = 'renren'
    allowed_domains = ['renren.com']
    # 个人中心页网址
    start_urls = ['http://www.renren.com/972990680/profile']

    def start_requests(self):
        # 登录之后用 chrome 的 debug 工具从请求中获取的 cookies
        cookiesstr = "anonymid=k3miegqc-hho317; depovince=ZGQT; _r01_=1; JSESSIONID=abcDdtGp7yEtG91r_U-6w; ick_login=d2631ff6-7b2d-4638-a2f5-c3a3f46b1595; ick=5499cd3f-c7a3-44ac-9146-60ac04440cb7; t=d1b681e8b5568a8f6140890d4f05c30f0; societyguester=d1b681e8b5568a8f6140890d4f05c30f0; id=972990680; xnsid=404266eb; XNESSESSIONID=62de8f52d318; jebecookies=4205498d-d0f7-4757-acd3-416f7aa0ae98|||||; ver=7.0; loginfrom=null; jebe_key=8800dc4d-e013-472b-a6aa-552ebfc11486%7Cb1a400326a5d6b2877f8c884e4fe9832%7C1575175011619%7C1%7C1575175011639; jebe_key=8800dc4d-e013-472b-a6aa-552ebfc11486%7Cb1a400326a5d6b2877f8c884e4fe9832%7C1575175011619%7C1%7C1575175011641; wp_fold=0"
        cookies = {i.split("=")[0]:i.split("=")[1] for i in cookiesstr.split("; ")}

        # 携带 cookies 的 Request 请求
        yield scrapy.Request(
            self.start_urls[0],
            callback=self.parse,
            cookies=cookies
        )

    def parse(self, response):
        # 从个人中心页查找关键词"闲欢"并打印
        print(re.findall("闲欢", response.body.decode()))
</code></pre></div></div>

<p>我先用账号正确登录人人网，登录之后用 chrome 的 debug 工具从请求中获取一个请求的 cookies ，然后在 <code class="highlighter-rouge">Request</code> 对象中加入这个 cookies 。接着我在 <code class="highlighter-rouge">parse</code> 方法中查找网页中的“闲欢”关键词并打印输出。</p>

<p>我们运行一下这个爬虫：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt;scrapy crawl renren
</code></pre></div></div>

<p>在运行日志中我们可以看到下面这几行：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2019-12-01 13:06:55 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://www.renren.com/972990680/profile?v=info_timeline&gt; (referer: http://www.renren.com/972990680/profile)
['闲欢', '闲欢', '闲欢', '闲欢', '闲欢', '闲欢', '闲欢']
2019-12-01 13:06:55 [scrapy.core.engine] INFO: Closing spider (finished)
</code></pre></div></div>
<p>我们可以看到已经打印了我们需要的信息了。</p>

<p>我们可以在 settings 配置中加 <code class="highlighter-rouge">COOKIES_DEBUG = True</code> 来查看 cookies 传递的过程。</p>

<p>加了这个配置之后，我们可以看到日志中会出现下面的信息：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2019-12-01 13:06:55 [scrapy.downloadermiddlewares.cookies] DEBUG: Sending cookies to: &lt;GET http://www.renren.com/972990680/profile?v=info_timeline&gt;
Cookie: anonymid=k3miegqc-hho317; depovince=ZGQT; _r01_=1; JSESSIONID=abcDdtGp7yEtG91r_U-6w; ick_login=d2631ff6-7b2d-4638-a2f5-c3a3f46b1595; ick=5499cd3f-c7a3-44ac-9146-60ac04440cb7; t=d1b681e8b5568a8f6140890d4f05c30f0; societyguester=d1b681e8b5568a8f6140890d4f05c30f0; id=972990680; xnsid=404266eb; XNESSESSIONID=62de8f52d318; jebecookies=4205498d-d0f7-4757-acd3-416f7aa0ae98|||||; ver=7.0; loginfrom=null; jebe_key=8800dc4d-e013-472b-a6aa-552ebfc11486%7Cb1a400326a5d6b2877f8c884e4fe9832%7C1575175011619%7C1%7C1575175011641; wp_fold=0; JSESSIONID=abc84VF0a7DUL7JcS2-6w
</code></pre></div></div>

<h3 id="发送-post-请求模拟登陆">发送 post 请求模拟登陆</h3>

<p>我们通过模拟登陆 GitHub 网站为例，来讲述这种模拟登陆方式。</p>

<p>我们首先创建一个爬虫 github ：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; scrapy genspider github github.com
</code></pre></div></div>

<p>我们要用 post 请求模拟登陆，首先需要知道登陆的 URL 地址，以及登陆所需要的参数信息。我们通过 debug 工具，可以看到登陆的请求信息如下：</p>

<p><img src="http://www.justdopython.com/assets/images/2019/python/github_login_request.png" alt="" /></p>

<p>从请求信息中我们可以找出登陆的 URL 为：<code class="highlighter-rouge">https://github.com/session</code> ，登陆所需要的参数为：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>commit: Sign in
utf8: ✓
authenticity_token: bbpX85KY36B7N6qJadpROzoEdiiMI6qQ5L7hYFdPS+zuNNFSKwbW8kAGW5ICyvNVuuY5FImLdArG47358RwhWQ==
ga_id: 101235085.1574734122
login: xxx@qq.com
password: xxx
webauthn-support: supported
webauthn-iuvpaa-support: unsupported
required_field_f0e5: 
timestamp: 1575184710948
timestamp_secret: 574aa2760765c42c07d9f0ad0bbfd9221135c3273172323d846016f43ba761db
</code></pre></div></div>

<p>这个请求的参数真是够多的，汗！</p>

<p>除了我们的用户名和密码，其他的都需要从登陆页面中获取，这其中还有一个 <code class="highlighter-rouge">required_field_f0e5</code> 参数需要注意一下，每次页面加载这个名词都不一样，可见是动态生成的，但是这个值始终传的都是空，这就为我们省去了一个参数，我们可以不穿这个参数。</p>

<p>其他的参数在页面的位置如下图：</p>

<p><img src="http://www.justdopython.com/assets/images/2019/python/github_login_params.png" alt="" /></p>

<p>我们用 xpath 来获取各个参数，代码如下（我把用户名和密码分别用 <code class="highlighter-rouge">xxx</code> 来代替了，大家运行的时候请把自己真实的用户名和密码写上去）：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># -*- coding: utf-8 -*-
import scrapy
import re

class GithubSpider(scrapy.Spider):
    name = 'github'
    allowed_domains = ['github.com']
    # 登录页面 URL
    start_urls = ['https://github.com/login']

    def parse(self, response):
        # 获取请求参数
        commit = response.xpath("//input[@name='commit']/@value").extract_first()
        utf8 = response.xpath("//input[@name='utf8']/@value").extract_first()
        authenticity_token = response.xpath("//input[@name='authenticity_token']/@value").extract_first()
        ga_id = response.xpath("//input[@name='ga_id']/@value").extract_first()
        webauthn_support = response.xpath("//input[@name='webauthn-support']/@value").extract_first()
        webauthn_iuvpaa_support = response.xpath("//input[@name='webauthn-iuvpaa-support']/@value").extract_first()
        # required_field_157f = response.xpath("//input[@name='required_field_4ed5']/@value").extract_first()
        timestamp = response.xpath("//input[@name='timestamp']/@value").extract_first()
        timestamp_secret = response.xpath("//input[@name='timestamp_secret']/@value").extract_first()

        # 构造 post 参数
        post_data = {
            "commit": commit,
            "utf8": utf8,
            "authenticity_token": authenticity_token,
            "ga_id": ga_id,
            "login": "xxx@qq.com",
            "password": "xxx",
            "webauthn-support": webauthn_support,
            "webauthn-iuvpaa-support": webauthn_iuvpaa_support,
            # "required_field_4ed5": required_field_4ed5,
            "timestamp": timestamp,
            "timestamp_secret": timestamp_secret
        }

        # 打印参数
        print(post_data)

        # 发送 post 请求
        yield scrapy.FormRequest(
            "https://github.com/session", # 登录请求方法
            formdata=post_data,
            callback=self.after_login
        )

    # 登录成功之后操作
    def after_login(self, response):
        # 找到页面上的 Issues 字段并打印
        print(re.findall("Issues", response.body.decode()))
</code></pre></div></div>

<p>我们使用 <code class="highlighter-rouge">FormRequest</code> 方法发送 post 请求，运行爬虫之后，报错了，我们来看下报错信息：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2019-12-01 15:14:47 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://github.com/login&gt; (referer: None)
{'commit': 'Sign in', 'utf8': '✓', 'authenticity_token': '3P4EVfXq3WvBM8fvWge7FfmRd0ORFlS6xGcz5mR5A00XnMe7GhFaMKQ8y024Hyy5r/RFS9ZErUDr1YwhDpBxlQ==', 'ga_id': None, 'login': '965639190@qq.com', 'password': '54ithero', 'webauthn-support': 'unknown', 'webauthn-iuvpaa-support': 'unknown', 'timestamp': '1575184487447', 'timestamp_secret': '6a8b589266e21888a4635ab0560304d53e7e8667d5da37933844acd7bee3cd19'}
2019-12-01 15:14:47 [scrapy.core.scraper] ERROR: Spider error processing &lt;GET https://github.com/login&gt; (referer: None)
Traceback (most recent call last):
  File "/Applications/anaconda3/lib/python3.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Applications/anaconda3/lib/python3.7/site-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/Applications/anaconda3/lib/python3.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Applications/anaconda3/lib/python3.7/site-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/Applications/anaconda3/lib/python3.7/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in &lt;genexpr&gt;
    return (_set_referer(r) for r in result or ())
  File "/Applications/anaconda3/lib/python3.7/site-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/Applications/anaconda3/lib/python3.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in &lt;genexpr&gt;
    return (r for r in result or () if _filter(r))
  File "/Applications/anaconda3/lib/python3.7/site-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/Applications/anaconda3/lib/python3.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in &lt;genexpr&gt;
    return (r for r in result or () if _filter(r))
  File "/Users/cxhuan/Documents/python_workspace/scrapy_projects/login/login/spiders/github.py", line 40, in parse
    callback=self.after_login
  File "/Applications/anaconda3/lib/python3.7/site-packages/scrapy/http/request/form.py", line 32, in __init__
    querystr = _urlencode(items, self.encoding)
  File "/Applications/anaconda3/lib/python3.7/site-packages/scrapy/http/request/form.py", line 73, in _urlencode
    for k, vs in seq
  File "/Applications/anaconda3/lib/python3.7/site-packages/scrapy/http/request/form.py", line 74, in &lt;listcomp&gt;
    for v in (vs if is_listlike(vs) else [vs])]
  File "/Applications/anaconda3/lib/python3.7/site-packages/scrapy/utils/python.py", line 107, in to_bytes
    'object, got %s' % type(text).__name__)
TypeError: to_bytes must receive a unicode, str or bytes object, got NoneType
2019-12-01 15:14:47 [scrapy.core.engine] INFO: Closing spider (finished)
</code></pre></div></div>

<p>看这个报错信息，好像是参数值中有一个参数取到 <code class="highlighter-rouge">None</code> 导致的，我们看下打印的参数信息中，发现 <code class="highlighter-rouge">ga_id</code> 是 <code class="highlighter-rouge">None</code> ，我们再修改一下，当 <code class="highlighter-rouge">ga_id</code> 为 <code class="highlighter-rouge">None</code> 时，我们传空字符串试试。</p>

<p>修改代码如下：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ga_id = response.xpath("//input[@name='ga_id']/@value").extract_first()
if ga_id is None:
    ga_id = ""
</code></pre></div></div>

<p>再次运行爬虫，这次我们来看看结果：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Set-Cookie: _gh_sess=QmtQRjB4UDNUeHdkcnE4TUxGbVRDcG9xMXFxclA1SDM3WVhqbFF5U0wwVFp0aGV1UWxYRWFSaXVrZEl0RnVjTzFhM1RrdUVabDhqQldTK3k3TEd3KzNXSzgvRXlVZncvdnpURVVNYmtON0IrcGw1SXF6Nnl0VTVDM2dVVGlsN01pWXNUeU5XQi9MbTdZU0lTREpEMllVcTBmVmV2b210Sm5Sbnc0N2d5aVErbjVDU2JCQnA5SkRsbDZtSzVlamxBbjdvWDBYaWlpcVR4Q2NvY3hwVUIyZz09LS1lMUlBcTlvU0F0K25UQ3loNHFOZExnPT0%3D--8764e6d2279a0e6960577a66864e6018ef213b56; path=/; secure; HttpOnly

2019-12-01 15:25:18 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://github.com/&gt; (referer: https://github.com/login)
['Issues', 'Issues']
2019-12-01 15:25:18 [scrapy.core.engine] INFO: Closing spider (finished)
</code></pre></div></div>

<p>我们可以看到已经打印了我们需要的信息，登录成功。</p>

<p>Scrapy 对于表单请求，<code class="highlighter-rouge">FormRequest</code> 还提供了另外一个方法 <code class="highlighter-rouge">from_response</code> 来自动获取页面中的表单，我们只需要传入用户名和密码就可以发送请求。</p>

<p>我们来看下这个方法的源码：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@classmethod
    def from_response(cls, response, formname=None, formid=None, formnumber=0, formdata=None,
                      clickdata=None, dont_click=False, formxpath=None, formcss=None, **kwargs):

        kwargs.setdefault('encoding', response.encoding)

        if formcss is not None:
            from parsel.csstranslator import HTMLTranslator
            formxpath = HTMLTranslator().css_to_xpath(formcss)

        form = _get_form(response, formname, formid, formnumber, formxpath)
        formdata = _get_inputs(form, formdata, dont_click, clickdata, response)
        url = _get_form_url(form, kwargs.pop('url', None))

        method = kwargs.pop('method', form.method)
        if method is not None:
            method = method.upper()
            if method not in cls.valid_form_methods:
                method = 'GET'

        return cls(url=url, method=method, formdata=formdata, **kwargs)
</code></pre></div></div>

<p>我们可以看到这个方法的参数有好多，都是有关 form 定位的信息。如果登录网页中只有一个表单， Scrapy 可以很容易定位，但是如果网页中含有多个表单呢？这个时候我们就需要通过这些参数来告诉 Scrapy 哪个才是登录的表单。</p>

<p>当然，这个方法的前提是需要我们网页的 form 表单的 action 里面包含了提交请求的 url 地址。</p>

<p>在 github 这个例子中，我们的登录页面只有一个登录的表单，因此我们只需要传入用户名和密码就可以了。代码如下：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># -*- coding: utf-8 -*-
import scrapy
import re

class Github2Spider(scrapy.Spider):
    name = 'github2'
    allowed_domains = ['github.com']
    start_urls = ['http://github.com/login']

    def parse(self, response):
        yield scrapy.FormRequest.from_response(
            response, # 自动从response中寻找form表单
            formdata={"login": "xxx@qq.com", "password": "xxx"},
            callback=self.after_login
        )
    # 登录成功之后操作
    def after_login(self, response):
        # 找到页面上的 Issues 字段并打印
        print(re.findall("Issues", response.body.decode()))
</code></pre></div></div>

<p>运行爬虫后，我们可以看到和之前一样的结果。</p>

<p>这种请求方式是不是简单了许多？不需要我们费力去找各种请求参数，有没有觉得 Amazing ？</p>

<h2 id="总结">总结</h2>

<p>本文向大家介绍了 Scrapy 模拟登陆网站的几种方法，大家可以自己运用文中的方法去实践一下。当然，这里没有涉及到有验证码的情况，验证码是一个复杂并且难度很高的专题，以后有时间再给大家介绍。</p>

<blockquote>
  <p>文中示例代码：<a href="https://github.com/JustDoPython/python-100-day">python-100-days</a></p>
</blockquote>


            <!-- <div>
   <p align="center">
     	  
         <img src="http://favorites.ren/assets/images/python.jpg" >
         <br/>
         微信扫描二维码，关注我的公众号
        

   </p>
   <p align="center" style="margin-top: 15px; font-size: 11px;color: #cc0000;">
       <strong>（转载本站文章请注明作者和出处 <a href="http://www.ityouknow.com">纯洁的微笑-ityouknow</a>）</strong>
   </p>
   <p align="center" style="margin-top: 15px; font-size: 16px;color: #337ab7;">
       <strong><a href="http://laughyouth.com/" target="_blank">点击了解：全网唯二以程序员为主题的漫画公众号</a></strong>
   </p>
</div> -->

            <!-- Comments -->
            <div class="comment">
             

  
    <a href="#" class="show_disqus_comment" onclick="return false;">Show Disqus Comments</a>
    <div id="disqus_thread"></div>
    <script>
    var disqus_config = function () {
        this.page.url = 'https://panshuai1121.github.io/python/2019/12/02/python-scrapy-login-076.html';
        this.page.identifier = '/python/2019/12/02/python-scrapy-login-076.html';
        this.page.title = '第76天：Scrapy 模拟登陆';
    };
    var disqus_loaded = false;
    $(function() {
        $('.show_disqus_comment').on('click', function() { // DON'T EDIT BELOW THIS LINE
            $(this).html('加载中...');
            var that = this;
            if (!disqus_loaded) {
                var d = document, s = d.createElement('script');

                s.type = 'text/javascript';
                s.async = true;
                var shortname = 'panshuai1121';

                s.src = '//' + shortname + '.disqus.com/embed.js';

                s.setAttribute('data-timestamp', +new Date());
                (d.head || d.body).appendChild(s);

                disqus_loaded = true;
            }
            $(that).remove();
        })
    })
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
  

  
        <div id="gitalk-container"></div>
        <script src="/assets/js/gitalk.min.js"></script>
        <script>
        var gitalk = new Gitalk({
            id: '/python/2019/12/02/python-scrapy-login-076.html',
            clientID: '01c69b4bdbdbc141e10d',
            clientSecret: '287c9fc8219a6ceba7428bbd9f3aa0e123e193fb',
            repo: 'blog-comments',
            owner: 'panshuai1121',
            admin: ['panshuai1121'],
            labels: ['gitment'],
            perPage: 50,
        })
        gitalk.render('gitalk-container')
        </script>
  


            </div>
        </div>

        <div class="col-md-3">
            <h3>Post Directory</h3>
<div id="post-directory-module">
<section class="post-directory">
    <!-- Links that trigger the jumping -->
    <!-- Added by javascript below -->
    <dl></dl>
</section>
</div>

<script type="text/javascript">

    $(document).ready(function(){
        $( "article h2" ).each(function( index ) {
            $(".post-directory dl").append("<dt><a class=\"jumper\" hre=#" +
                    $(this).attr("id")
                    + ">"
                    + $(this).text()
                    + "</a></dt>");

            var children = $(this).nextUntil("h2", "h3")

            children.each(function( index ) {
                $(".post-directory dl").append("<dd><a class=\"jumper\" hre=#" +
                        $(this).attr("id")
                        + ">"
                        + "&nbsp;&nbsp;- " + $(this).text()
                        + "</a></dd>");
            });
        });

        var fixmeTop = $('#post-directory-module').offset().top - 100;       // get initial position of the element

        $(window).scroll(function() {                  // assign scroll event listener

            var currentScroll = $(window).scrollTop(); // get current position

            if (currentScroll >= fixmeTop) {           // apply position: fixed if you
                $('#post-directory-module').css({      // scroll to that element or below it
                    top: '100px',
                    position: 'fixed',
                    width: 'inherit'
                });
            } else {                                   // apply position: static
                $('#post-directory-module').css({      // if you scroll above it
                    position: 'inherit',
                    width: 'inherit'
                });
            }

        });

        $("a.jumper").on("click", function( e ) {

            e.preventDefault();

            $("body, html").animate({
                scrollTop: ($( $(this).attr('hre') ).offset().top - 100)
            }, 600);

        });
    });

</script>
        </div>
        

    </div>
     <!-- <div class="asb-post-01">
        <div class="mask"></div>
            <div class="info">
                <div>扫码关注公众号：<span style="color: #E9405A; font-weight: bold;">纯洁的微笑</span></div>
            <div>
            <span>发送 </span><span class="token" style="color: #e9415a; font-weight: bold; font-size: 17px; margin-bottom: 45px;">290992</span>
            <div>
                即可<span style="color: #e9415a; font-weight: bold;">立即永久</span>解锁本站全部文章
            </div>
            <img class="code-img" style="width: 300px;display:unset" src="http://favorites.ren/assets/images/keeppuresmile.jpg">
        </div>
    </div> -->
</article>

        </div>

    
<footer class="container">
    <div class="site-footer">
       <!--  <div class="site-footer-icons">
            <a target="_blank" href="https://www.zhihu.com/people/ityouknow">
               知乎
            </a>
            <a target="_blank" href="http://weibo.com/ityouknow">
               微博
            </a>
            <a target="_blank" href="https://github.com/ityouknow">
                GitHub
            </a>
        </div> -->
          <!-- <div class="card text-center">
            <ul class="list-inline" style="margin-left: 0;">
               <li>
                <a target="_blank" href="https://github.com/panshuai1121">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
                <a target="_blank" href="https://www.zhihu.com/people/ityouknow">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa  fa-stack-1x fa-inverse">知</i>
                  </span>
                </a>
              </li>
                  <li>
                <a target="_blank" href="https://weibo.com/ityouknow">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-weibo fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
          </ul>
        </div> -->
        <div class="site-footer-links mobile-hidden">
            
            <a href="/payment.html" title="Pay" target="_self">Pay</a>
            
            <a href="/fastdfs.html" title="FastDFS" target="_blank">FastDFS</a>
            
            <a href="/mongodb.html" title="MongoDB" target="_blank">MongoDB</a>
            
            <a href="/docker.html" title="Docker" target="_blank">Docker</a>
            
            <a href="/open-source.html" title="Code" target="_blank">Code</a>
            
            <a href="/gitchat.html" title="Chat" target="_blank">Chat</a>
            
            <a href="/redis.html" title="redis" target="_blank">redis</a>
            
        </div>
        <div class="site-footer-icons">
               <a href="http://beian.miit.gov.cn" >京ICP备15067287号</a>
        </div>
        <div class="scrolltop">
            <a href="javascript:window.scrollTo(0,0)" >TOP</a>
        </div>
        <div class="rss">
            <a href="/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a>
            Power by <a href="https://github.com/DONGChuan/Yummy-Jekyll">Yummy Jekyll</a>
        </div>
    </div>
    <!-- Third-Party JS -->
    <script type="text/javascript" src="http://favorites.ren/bower_components/geopattern/js/geopattern.min.js"></script>
    <!-- My JS -->
    <script type="text/javascript" src="http://favorites.ren/assets/js/script.js"></script>
    
    
     <!-- Cnzz Analytics -->
       <!-- <div style="display:none">
         <script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1260945749'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s4.cnzz.com/z_stat.php%3Fid%3D1260945749' type='text/javascript'%3E%3C/script%3E"));</script>
       </div> -->
     <!-- Cnzz Analytics -->
</footer>


    </body>

</html>
