I"ÔV<p>by é—²æ¬¢</p>

<p>æƒ³çˆ¬å–ç½‘ç«™æ•°æ®ï¼Ÿå…ˆç™»å½•ç½‘ç«™ï¼å¯¹äºå¤§å¤šæ•°å¤§å‹ç½‘ç«™æ¥è¯´ï¼Œæƒ³è¦çˆ¬å–ä»–ä»¬çš„æ•°æ®ï¼Œç¬¬ä¸€é“é—¨æ§›å°±æ˜¯ç™»å½•ç½‘ç«™ã€‚ä¸‹é¢è¯·è·Ÿéšæˆ‘çš„æ­¥ä¼æ¥å­¦ä¹ å¦‚ä½•æ¨¡æ‹Ÿç™»é™†ç½‘ç«™ã€‚
<!--more--></p>

<h2 id="ä¸ºä»€ä¹ˆè¿›è¡Œæ¨¡æ‹Ÿç™»é™†">ä¸ºä»€ä¹ˆè¿›è¡Œæ¨¡æ‹Ÿç™»é™†ï¼Ÿ</h2>

<p>äº’è”ç½‘ä¸Šçš„ç½‘ç«™åˆ†ä¸¤ç§ï¼šéœ€è¦ç™»å½•å’Œä¸éœ€è¦ç™»å½•ã€‚ï¼ˆè¿™æ˜¯ä¸€å¥åºŸè¯ï¼ï¼‰</p>

<p>é‚£ä¹ˆï¼Œå¯¹äºä¸éœ€è¦ç™»å½•çš„ç½‘ç«™ï¼Œæˆ‘ä»¬ç›´æ¥è·å–æ•°æ®å³å¯ï¼Œç®€å•çœäº‹ã€‚è€Œå¯¹äºéœ€è¦ç™»å½•æ‰å¯ä»¥æŸ¥çœ‹æ•°æ®æˆ–è€…ä¸ç™»å½•åªèƒ½æŸ¥çœ‹ä¸€éƒ¨åˆ†æ•°æ®çš„ç½‘ç«™æ¥è¯´ï¼Œæˆ‘ä»¬åªå¥½ä¹–ä¹–åœ°ç™»å½•ç½‘ç«™äº†ã€‚ï¼ˆé™¤éä½ ç›´æ¥é»‘è¿›äººå®¶æ•°æ®åº“ï¼Œé»‘å®¢æ“ä½œè¯·æ…ç”¨ï¼ï¼‰</p>

<p>æ‰€ä»¥ï¼Œå¯¹äºéœ€è¦ç™»å½•çš„ç½‘ç«™ï¼Œæˆ‘ä»¬éœ€è¦æ¨¡æ‹Ÿä¸€ä¸‹ç™»å½•ï¼Œä¸€æ–¹é¢ä¸ºäº†è·å–ç™»é™†ä¹‹åé¡µé¢çš„ä¿¡æ¯å’Œæ•°æ®ï¼Œå¦ä¸€æ–¹é¢ä¸ºäº†æ‹¿åˆ°ç™»å½•ä¹‹åçš„ cookie ï¼Œä»¥ä¾¿ä¸‹æ¬¡è¯·æ±‚æ—¶ä½¿ç”¨ã€‚</p>

<h2 id="æ¨¡æ‹Ÿç™»é™†çš„æ€è·¯">æ¨¡æ‹Ÿç™»é™†çš„æ€è·¯</h2>

<p>ä¸€æåˆ°æ¨¡æ‹Ÿç™»é™†ï¼Œå¤§å®¶çš„ç¬¬ä¸€ååº”è‚¯å®šæ˜¯ï¼šåˆ‡ï¼é‚£è¿˜ä¸ç®€å•ï¼Ÿæ‰“å¼€æµè§ˆå™¨ï¼Œè¾“å…¥ç½‘å€ï¼Œæ‰¾åˆ°ç”¨æˆ·åå¯†ç æ¡†ï¼Œè¾“å…¥ç”¨æˆ·åå’Œå¯†ç ï¼Œç„¶åç‚¹å‡»ç™»é™†å°±å®Œäº‹ï¼</p>

<p>è¿™ç§æ–¹å¼æ²¡æ¯›ç—…ï¼Œæˆ‘ä»¬çš„ selenium æ¨¡æ‹Ÿç™»é™†å°±æ˜¯è¿™ä¹ˆæ“ä½œçš„ã€‚</p>

<p>é™¤æ­¤ä¹‹å¤–å‘¢ï¼Œæˆ‘ä»¬çš„ Requests è¿˜å¯ä»¥ç›´æ¥æºå¸¦å·²ç»ç™»é™†è¿‡çš„ cookies è¿›è¡Œè¯·æ±‚ï¼Œç›¸å½“äºç»•è¿‡äº†ç™»é™†ã€‚</p>

<p>æˆ‘ä»¬ä¹Ÿå¯ä»¥åˆ©ç”¨ Requests å‘é€ post è¯·æ±‚ï¼Œå°†ç½‘ç«™ç™»å½•éœ€è¦çš„ä¿¡æ¯é™„å¸¦åˆ° post è¯·æ±‚ä¸­è¿›è¡Œç™»å½•ã€‚</p>

<p>ä»¥ä¸Šå°±æ˜¯æˆ‘ä»¬å¸¸è§çš„ä¸‰ç§æ¨¡æ‹Ÿç™»é™†ç½‘ç«™çš„æ€è·¯ï¼Œé‚£ä¹ˆæˆ‘ä»¬çš„ Scrapy ä¹Ÿä½¿ç”¨äº†åä¸¤ç§æ–¹å¼ï¼Œæ¯•ç«Ÿç¬¬ä¸€ç§åªæ˜¯ selenium ç‰¹æœ‰çš„æ–¹å¼ã€‚</p>

<p>Scrapy æ¨¡æ‹Ÿç™»é™†çš„æ€è·¯ï¼š</p>

<blockquote>
  <p>1ã€ç›´æ¥æºå¸¦å·²ç»ç™»é™†è¿‡çš„ cookies è¿›è¡Œè¯·æ±‚<br />
2ã€å°†ç½‘ç«™ç™»å½•éœ€è¦çš„ä¿¡æ¯é™„å¸¦åˆ° post è¯·æ±‚ä¸­è¿›è¡Œç™»å½•</p>
</blockquote>

<h2 id="æ¨¡æ‹Ÿç™»é™†å®ä¾‹">æ¨¡æ‹Ÿç™»é™†å®ä¾‹</h2>

<h3 id="æºå¸¦-cookies-æ¨¡æ‹Ÿç™»é™†">æºå¸¦ cookies æ¨¡æ‹Ÿç™»é™†</h3>
<p>æ¯ç§ç™»é™†æ–¹å¼éƒ½æœ‰å®ƒçš„ä¼˜ç¼ºç‚¹ä»¥åŠä½¿ç”¨åœºæ™¯ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹æºå¸¦ cookies ç™»é™†çš„åº”ç”¨åœºæ™¯ï¼š</p>

<blockquote>
  <p>1ã€cookie è¿‡æœŸæ—¶é—´å¾ˆé•¿ï¼Œæˆ‘ä»¬å¯ä»¥ç™»å½•ä¸€æ¬¡ä¹‹åä¸ç”¨æ‹…å¿ƒç™»å½•è¿‡æœŸé—®é¢˜ï¼Œå¸¸è§äºä¸€äº›ä¸è§„èŒƒçš„ç½‘ç«™ã€‚<br />
2ã€æˆ‘ä»¬èƒ½åœ¨ cookie è¿‡æœŸä¹‹å‰æŠŠæˆ‘ä»¬éœ€è¦çš„æ‰€æœ‰æ•°æ®æ‹¿åˆ°ã€‚<br />
3ã€æˆ‘ä»¬å¯ä»¥é…åˆå…¶ä»–ç¨‹åºä½¿ç”¨ï¼Œæ¯”å¦‚ä½¿ç”¨ selenium æŠŠç™»å½•ä¹‹åçš„ cookie è·å–ä¿å­˜åˆ°æœ¬åœ°ï¼Œç„¶ååœ¨ Scrapy å‘é€è¯·æ±‚ä¹‹å‰å…ˆè¯»å–æœ¬åœ° cookie ã€‚</p>
</blockquote>

<p>ä¸‹é¢æˆ‘ä»¬é€šè¿‡æ¨¡æ‹Ÿç™»å½•è¢«æˆ‘ä»¬é—å¿˜å·²ä¹…çš„äººäººç½‘æ¥è®²è¿°è¿™ç§æ¨¡æ‹Ÿç™»é™†æ–¹å¼ã€‚</p>

<p>æˆ‘ä»¬é¦–å…ˆåˆ›å»ºä¸€ä¸ª Scrapy é¡¹ç›®ï¼š</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; scrapy startproject login
</code></pre></div></div>

<p>ä¸ºäº†çˆ¬å–é¡ºåˆ©ï¼Œè¯·å…ˆå°† settings é‡Œé¢çš„ robots åè®®è®¾ç½®ä¸º False ï¼š</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ROBOTSTXT_OBEY = False
</code></pre></div></div>

<p>æ¥ç€ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªçˆ¬è™«ï¼š</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; scrapy genspider renren renren.com
</code></pre></div></div>
<p>æˆ‘ä»¬æ‰“å¼€ spiders ç›®å½•ä¸‹çš„ renren.py ï¼Œä»£ç å¦‚ä¸‹ï¼š</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># -*- coding: utf-8 -*-
import scrapy


class RenrenSpider(scrapy.Spider):
    name = 'renren'
    allowed_domains = ['renren.com']
    start_urls = ['http://renren.com/']

    def parse(self, response):
        pass

</code></pre></div></div>

<p>æˆ‘ä»¬çŸ¥é“ï¼Œ<code class="highlighter-rouge">start_urls</code> å­˜çš„æ˜¯æˆ‘ä»¬éœ€è¦çˆ¬å–çš„ç¬¬ä¸€ä¸ªç½‘é¡µåœ°å€ï¼Œè¿™æ˜¯æˆ‘ä»¬çˆ¬æ•°æ®çš„åˆå§‹ç½‘é¡µï¼Œå‡è®¾æˆ‘éœ€è¦çˆ¬å–äººäººç½‘çš„ä¸ªäººä¸­å¿ƒé¡µçš„æ•°æ®ï¼Œé‚£ä¹ˆæˆ‘ç™»å½•äººäººç½‘åï¼Œè¿›å…¥åˆ°ä¸ªäººä¸­å¿ƒé¡µï¼Œç½‘å€æ˜¯ï¼š<code class="highlighter-rouge">http://www.renren.com/972990680/profile</code> ï¼Œå¦‚æœæˆ‘ç›´æ¥å°†è¿™ä¸ªç½‘å€æ”¾åˆ° <code class="highlighter-rouge">start_urls</code>  é‡Œé¢ï¼Œç„¶åæˆ‘ä»¬ç›´æ¥è¯·æ±‚ï¼Œå¤§å®¶æƒ³ä¸€ä¸‹ï¼Œå¯ä¸å¯ä»¥æˆåŠŸï¼Ÿ</p>

<p>ä¸å¯ä»¥ï¼Œå¯¹å§ï¼å› ä¸ºæˆ‘ä»¬è¿˜æ²¡æœ‰ç™»å½•ï¼Œæ ¹æœ¬çœ‹ä¸åˆ°ä¸ªäººä¸­å¿ƒé¡µã€‚</p>

<p>é‚£ä¹ˆæˆ‘ä»¬çš„ç™»å½•ä»£ç åŠ åˆ°å“ªé‡Œå‘¢ï¼Ÿ</p>

<p>æˆ‘ä»¬èƒ½ç¡®å®šçš„æ˜¯æˆ‘ä»¬å¿…é¡»åœ¨æ¡†æ¶è¯·æ±‚ <code class="highlighter-rouge">start_urls</code> ä¸­çš„ç½‘é¡µä¹‹å‰ç™»å½•ã€‚</p>

<p>æˆ‘ä»¬è¿›å…¥ Spider ç±»çš„æºç ï¼Œæ‰¾åˆ°ä¸‹é¢è¿™ä¸€æ®µä»£ç ï¼š</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def start_requests(self):
        cls = self.__class__
        if method_is_overridden(cls, Spider, 'make_requests_from_url'):
            warnings.warn(
                "Spider.make_requests_from_url method is deprecated; it "
                "won't be called in future Scrapy releases. Please "
                "override Spider.start_requests method instead (see %s.%s)." % (
                    cls.__module__, cls.__name__
                ),
            )
            for url in self.start_urls:
                yield self.make_requests_from_url(url)
        else:
            for url in self.start_urls:
                yield Request(url, dont_filter=True)

    def make_requests_from_url(self, url):
        """ This method is deprecated. """
        return Request(url, dont_filter=True)

</code></pre></div></div>

<p>æˆ‘ä»¬ä»è¿™æ®µæºç ä¸­å¯ä»¥çœ‹åˆ°ï¼Œè¿™ä¸ªæ–¹æ³•ä» <code class="highlighter-rouge">start_urls</code> ä¸­è·å– URL ï¼Œç„¶åæ„é€ ä¸€ä¸ª Request å¯¹è±¡æ¥è¯·æ±‚ã€‚æ—¢ç„¶è¿™æ ·ï¼Œæˆ‘ä»¬å°±å¯ä»¥é‡å†™ <code class="highlighter-rouge">start_requests</code> æ–¹æ³•æ¥åšä¸€äº›äº‹æƒ…ï¼Œä¹Ÿå°±æ˜¯åœ¨æ„é€  <code class="highlighter-rouge">Request</code> å¯¹è±¡çš„æ—¶å€™æŠŠ cookies ä¿¡æ¯åŠ è¿›å»ã€‚</p>

<p>é‡å†™ä¹‹åçš„ <code class="highlighter-rouge">start_requests</code> æ–¹æ³•å¦‚ä¸‹ï¼š</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># -*- coding: utf-8 -*-
import scrapy
import re

class RenrenSpider(scrapy.Spider):
    name = 'renren'
    allowed_domains = ['renren.com']
    # ä¸ªäººä¸­å¿ƒé¡µç½‘å€
    start_urls = ['http://www.renren.com/972990680/profile']

    def start_requests(self):
        # ç™»å½•ä¹‹åç”¨ chrome çš„ debug å·¥å…·ä»è¯·æ±‚ä¸­è·å–çš„ cookies
        cookiesstr = "anonymid=k3miegqc-hho317; depovince=ZGQT; _r01_=1; JSESSIONID=abcDdtGp7yEtG91r_U-6w; ick_login=d2631ff6-7b2d-4638-a2f5-c3a3f46b1595; ick=5499cd3f-c7a3-44ac-9146-60ac04440cb7; t=d1b681e8b5568a8f6140890d4f05c30f0; societyguester=d1b681e8b5568a8f6140890d4f05c30f0; id=972990680; xnsid=404266eb; XNESSESSIONID=62de8f52d318; jebecookies=4205498d-d0f7-4757-acd3-416f7aa0ae98|||||; ver=7.0; loginfrom=null; jebe_key=8800dc4d-e013-472b-a6aa-552ebfc11486%7Cb1a400326a5d6b2877f8c884e4fe9832%7C1575175011619%7C1%7C1575175011639; jebe_key=8800dc4d-e013-472b-a6aa-552ebfc11486%7Cb1a400326a5d6b2877f8c884e4fe9832%7C1575175011619%7C1%7C1575175011641; wp_fold=0"
        cookies = {i.split("=")[0]:i.split("=")[1] for i in cookiesstr.split("; ")}

        # æºå¸¦ cookies çš„ Request è¯·æ±‚
        yield scrapy.Request(
            self.start_urls[0],
            callback=self.parse,
            cookies=cookies
        )

    def parse(self, response):
        # ä»ä¸ªäººä¸­å¿ƒé¡µæŸ¥æ‰¾å…³é”®è¯"é—²æ¬¢"å¹¶æ‰“å°
        print(re.findall("é—²æ¬¢", response.body.decode()))
</code></pre></div></div>

<p>æˆ‘å…ˆç”¨è´¦å·æ­£ç¡®ç™»å½•äººäººç½‘ï¼Œç™»å½•ä¹‹åç”¨ chrome çš„ debug å·¥å…·ä»è¯·æ±‚ä¸­è·å–ä¸€ä¸ªè¯·æ±‚çš„ cookies ï¼Œç„¶ååœ¨ <code class="highlighter-rouge">Request</code> å¯¹è±¡ä¸­åŠ å…¥è¿™ä¸ª cookies ã€‚æ¥ç€æˆ‘åœ¨ <code class="highlighter-rouge">parse</code> æ–¹æ³•ä¸­æŸ¥æ‰¾ç½‘é¡µä¸­çš„â€œé—²æ¬¢â€å…³é”®è¯å¹¶æ‰“å°è¾“å‡ºã€‚</p>

<p>æˆ‘ä»¬è¿è¡Œä¸€ä¸‹è¿™ä¸ªçˆ¬è™«ï¼š</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt;scrapy crawl renren
</code></pre></div></div>

<p>åœ¨è¿è¡Œæ—¥å¿—ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸‹é¢è¿™å‡ è¡Œï¼š</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2019-12-01 13:06:55 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://www.renren.com/972990680/profile?v=info_timeline&gt; (referer: http://www.renren.com/972990680/profile)
['é—²æ¬¢', 'é—²æ¬¢', 'é—²æ¬¢', 'é—²æ¬¢', 'é—²æ¬¢', 'é—²æ¬¢', 'é—²æ¬¢']
2019-12-01 13:06:55 [scrapy.core.engine] INFO: Closing spider (finished)
</code></pre></div></div>
<p>æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å·²ç»æ‰“å°äº†æˆ‘ä»¬éœ€è¦çš„ä¿¡æ¯äº†ã€‚</p>

<p>æˆ‘ä»¬å¯ä»¥åœ¨ settings é…ç½®ä¸­åŠ  <code class="highlighter-rouge">COOKIES_DEBUG = True</code> æ¥æŸ¥çœ‹ cookies ä¼ é€’çš„è¿‡ç¨‹ã€‚</p>

<p>åŠ äº†è¿™ä¸ªé…ç½®ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ—¥å¿—ä¸­ä¼šå‡ºç°ä¸‹é¢çš„ä¿¡æ¯ï¼š</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2019-12-01 13:06:55 [scrapy.downloadermiddlewares.cookies] DEBUG: Sending cookies to: &lt;GET http://www.renren.com/972990680/profile?v=info_timeline&gt;
Cookie: anonymid=k3miegqc-hho317; depovince=ZGQT; _r01_=1; JSESSIONID=abcDdtGp7yEtG91r_U-6w; ick_login=d2631ff6-7b2d-4638-a2f5-c3a3f46b1595; ick=5499cd3f-c7a3-44ac-9146-60ac04440cb7; t=d1b681e8b5568a8f6140890d4f05c30f0; societyguester=d1b681e8b5568a8f6140890d4f05c30f0; id=972990680; xnsid=404266eb; XNESSESSIONID=62de8f52d318; jebecookies=4205498d-d0f7-4757-acd3-416f7aa0ae98|||||; ver=7.0; loginfrom=null; jebe_key=8800dc4d-e013-472b-a6aa-552ebfc11486%7Cb1a400326a5d6b2877f8c884e4fe9832%7C1575175011619%7C1%7C1575175011641; wp_fold=0; JSESSIONID=abc84VF0a7DUL7JcS2-6w
</code></pre></div></div>

<h3 id="å‘é€-post-è¯·æ±‚æ¨¡æ‹Ÿç™»é™†">å‘é€ post è¯·æ±‚æ¨¡æ‹Ÿç™»é™†</h3>

<p>æˆ‘ä»¬é€šè¿‡æ¨¡æ‹Ÿç™»é™† GitHub ç½‘ç«™ä¸ºä¾‹ï¼Œæ¥è®²è¿°è¿™ç§æ¨¡æ‹Ÿç™»é™†æ–¹å¼ã€‚</p>

<p>æˆ‘ä»¬é¦–å…ˆåˆ›å»ºä¸€ä¸ªçˆ¬è™« github ï¼š</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; scrapy genspider github github.com
</code></pre></div></div>

<p>æˆ‘ä»¬è¦ç”¨ post è¯·æ±‚æ¨¡æ‹Ÿç™»é™†ï¼Œé¦–å…ˆéœ€è¦çŸ¥é“ç™»é™†çš„ URL åœ°å€ï¼Œä»¥åŠç™»é™†æ‰€éœ€è¦çš„å‚æ•°ä¿¡æ¯ã€‚æˆ‘ä»¬é€šè¿‡ debug å·¥å…·ï¼Œå¯ä»¥çœ‹åˆ°ç™»é™†çš„è¯·æ±‚ä¿¡æ¯å¦‚ä¸‹ï¼š</p>

<p><img src="http://www.justdopython.com/assets/images/2019/python/github_login_request.png" alt="" /></p>

<p>ä»è¯·æ±‚ä¿¡æ¯ä¸­æˆ‘ä»¬å¯ä»¥æ‰¾å‡ºç™»é™†çš„ URL ä¸ºï¼š<code class="highlighter-rouge">https://github.com/session</code> ï¼Œç™»é™†æ‰€éœ€è¦çš„å‚æ•°ä¸ºï¼š</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>commit: Sign in
utf8: âœ“
authenticity_token: bbpX85KY36B7N6qJadpROzoEdiiMI6qQ5L7hYFdPS+zuNNFSKwbW8kAGW5ICyvNVuuY5FImLdArG47358RwhWQ==
ga_id: 101235085.1574734122
login: xxx@qq.com
password: xxx
webauthn-support: supported
webauthn-iuvpaa-support: unsupported
required_field_f0e5: 
timestamp: 1575184710948
timestamp_secret: 574aa2760765c42c07d9f0ad0bbfd9221135c3273172323d846016f43ba761db
</code></pre></div></div>

<p>è¿™ä¸ªè¯·æ±‚çš„å‚æ•°çœŸæ˜¯å¤Ÿå¤šçš„ï¼Œæ±—ï¼</p>

<p>é™¤äº†æˆ‘ä»¬çš„ç”¨æˆ·åå’Œå¯†ç ï¼Œå…¶ä»–çš„éƒ½éœ€è¦ä»ç™»é™†é¡µé¢ä¸­è·å–ï¼Œè¿™å…¶ä¸­è¿˜æœ‰ä¸€ä¸ª <code class="highlighter-rouge">required_field_f0e5</code> å‚æ•°éœ€è¦æ³¨æ„ä¸€ä¸‹ï¼Œæ¯æ¬¡é¡µé¢åŠ è½½è¿™ä¸ªåè¯éƒ½ä¸ä¸€æ ·ï¼Œå¯è§æ˜¯åŠ¨æ€ç”Ÿæˆçš„ï¼Œä½†æ˜¯è¿™ä¸ªå€¼å§‹ç»ˆä¼ çš„éƒ½æ˜¯ç©ºï¼Œè¿™å°±ä¸ºæˆ‘ä»¬çœå»äº†ä¸€ä¸ªå‚æ•°ï¼Œæˆ‘ä»¬å¯ä»¥ä¸ç©¿è¿™ä¸ªå‚æ•°ã€‚</p>

<p>å…¶ä»–çš„å‚æ•°åœ¨é¡µé¢çš„ä½ç½®å¦‚ä¸‹å›¾ï¼š</p>

<p><img src="http://www.justdopython.com/assets/images/2019/python/github_login_params.png" alt="" /></p>

<p>æˆ‘ä»¬ç”¨ xpath æ¥è·å–å„ä¸ªå‚æ•°ï¼Œä»£ç å¦‚ä¸‹ï¼ˆæˆ‘æŠŠç”¨æˆ·åå’Œå¯†ç åˆ†åˆ«ç”¨ <code class="highlighter-rouge">xxx</code> æ¥ä»£æ›¿äº†ï¼Œå¤§å®¶è¿è¡Œçš„æ—¶å€™è¯·æŠŠè‡ªå·±çœŸå®çš„ç”¨æˆ·åå’Œå¯†ç å†™ä¸Šå»ï¼‰ï¼š</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># -*- coding: utf-8 -*-
import scrapy
import re

class GithubSpider(scrapy.Spider):
    name = 'github'
    allowed_domains = ['github.com']
    # ç™»å½•é¡µé¢ URL
    start_urls = ['https://github.com/login']

    def parse(self, response):
        # è·å–è¯·æ±‚å‚æ•°
        commit = response.xpath("//input[@name='commit']/@value").extract_first()
        utf8 = response.xpath("//input[@name='utf8']/@value").extract_first()
        authenticity_token = response.xpath("//input[@name='authenticity_token']/@value").extract_first()
        ga_id = response.xpath("//input[@name='ga_id']/@value").extract_first()
        webauthn_support = response.xpath("//input[@name='webauthn-support']/@value").extract_first()
        webauthn_iuvpaa_support = response.xpath("//input[@name='webauthn-iuvpaa-support']/@value").extract_first()
        # required_field_157f = response.xpath("//input[@name='required_field_4ed5']/@value").extract_first()
        timestamp = response.xpath("//input[@name='timestamp']/@value").extract_first()
        timestamp_secret = response.xpath("//input[@name='timestamp_secret']/@value").extract_first()

        # æ„é€  post å‚æ•°
        post_data = {
            "commit": commit,
            "utf8": utf8,
            "authenticity_token": authenticity_token,
            "ga_id": ga_id,
            "login": "xxx@qq.com",
            "password": "xxx",
            "webauthn-support": webauthn_support,
            "webauthn-iuvpaa-support": webauthn_iuvpaa_support,
            # "required_field_4ed5": required_field_4ed5,
            "timestamp": timestamp,
            "timestamp_secret": timestamp_secret
        }

        # æ‰“å°å‚æ•°
        print(post_data)

        # å‘é€ post è¯·æ±‚
        yield scrapy.FormRequest(
            "https://github.com/session", # ç™»å½•è¯·æ±‚æ–¹æ³•
            formdata=post_data,
            callback=self.after_login
        )

    # ç™»å½•æˆåŠŸä¹‹åæ“ä½œ
    def after_login(self, response):
        # æ‰¾åˆ°é¡µé¢ä¸Šçš„ Issues å­—æ®µå¹¶æ‰“å°
        print(re.findall("Issues", response.body.decode()))
</code></pre></div></div>

<p>æˆ‘ä»¬ä½¿ç”¨ <code class="highlighter-rouge">FormRequest</code> æ–¹æ³•å‘é€ post è¯·æ±‚ï¼Œè¿è¡Œçˆ¬è™«ä¹‹åï¼ŒæŠ¥é”™äº†ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸‹æŠ¥é”™ä¿¡æ¯ï¼š</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2019-12-01 15:14:47 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://github.com/login&gt; (referer: None)
{'commit': 'Sign in', 'utf8': 'âœ“', 'authenticity_token': '3P4EVfXq3WvBM8fvWge7FfmRd0ORFlS6xGcz5mR5A00XnMe7GhFaMKQ8y024Hyy5r/RFS9ZErUDr1YwhDpBxlQ==', 'ga_id': None, 'login': '965639190@qq.com', 'password': '54ithero', 'webauthn-support': 'unknown', 'webauthn-iuvpaa-support': 'unknown', 'timestamp': '1575184487447', 'timestamp_secret': '6a8b589266e21888a4635ab0560304d53e7e8667d5da37933844acd7bee3cd19'}
2019-12-01 15:14:47 [scrapy.core.scraper] ERROR: Spider error processing &lt;GET https://github.com/login&gt; (referer: None)
Traceback (most recent call last):
  File "/Applications/anaconda3/lib/python3.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Applications/anaconda3/lib/python3.7/site-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/Applications/anaconda3/lib/python3.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Applications/anaconda3/lib/python3.7/site-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/Applications/anaconda3/lib/python3.7/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in &lt;genexpr&gt;
    return (_set_referer(r) for r in result or ())
  File "/Applications/anaconda3/lib/python3.7/site-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/Applications/anaconda3/lib/python3.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in &lt;genexpr&gt;
    return (r for r in result or () if _filter(r))
  File "/Applications/anaconda3/lib/python3.7/site-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/Applications/anaconda3/lib/python3.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in &lt;genexpr&gt;
    return (r for r in result or () if _filter(r))
  File "/Users/cxhuan/Documents/python_workspace/scrapy_projects/login/login/spiders/github.py", line 40, in parse
    callback=self.after_login
  File "/Applications/anaconda3/lib/python3.7/site-packages/scrapy/http/request/form.py", line 32, in __init__
    querystr = _urlencode(items, self.encoding)
  File "/Applications/anaconda3/lib/python3.7/site-packages/scrapy/http/request/form.py", line 73, in _urlencode
    for k, vs in seq
  File "/Applications/anaconda3/lib/python3.7/site-packages/scrapy/http/request/form.py", line 74, in &lt;listcomp&gt;
    for v in (vs if is_listlike(vs) else [vs])]
  File "/Applications/anaconda3/lib/python3.7/site-packages/scrapy/utils/python.py", line 107, in to_bytes
    'object, got %s' % type(text).__name__)
TypeError: to_bytes must receive a unicode, str or bytes object, got NoneType
2019-12-01 15:14:47 [scrapy.core.engine] INFO: Closing spider (finished)
</code></pre></div></div>

<p>çœ‹è¿™ä¸ªæŠ¥é”™ä¿¡æ¯ï¼Œå¥½åƒæ˜¯å‚æ•°å€¼ä¸­æœ‰ä¸€ä¸ªå‚æ•°å–åˆ° <code class="highlighter-rouge">None</code> å¯¼è‡´çš„ï¼Œæˆ‘ä»¬çœ‹ä¸‹æ‰“å°çš„å‚æ•°ä¿¡æ¯ä¸­ï¼Œå‘ç° <code class="highlighter-rouge">ga_id</code> æ˜¯ <code class="highlighter-rouge">None</code> ï¼Œæˆ‘ä»¬å†ä¿®æ”¹ä¸€ä¸‹ï¼Œå½“ <code class="highlighter-rouge">ga_id</code> ä¸º <code class="highlighter-rouge">None</code> æ—¶ï¼Œæˆ‘ä»¬ä¼ ç©ºå­—ç¬¦ä¸²è¯•è¯•ã€‚</p>

<p>ä¿®æ”¹ä»£ç å¦‚ä¸‹ï¼š</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ga_id = response.xpath("//input[@name='ga_id']/@value").extract_first()
if ga_id is None:
    ga_id = ""
</code></pre></div></div>

<p>å†æ¬¡è¿è¡Œçˆ¬è™«ï¼Œè¿™æ¬¡æˆ‘ä»¬æ¥çœ‹çœ‹ç»“æœï¼š</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Set-Cookie: _gh_sess=QmtQRjB4UDNUeHdkcnE4TUxGbVRDcG9xMXFxclA1SDM3WVhqbFF5U0wwVFp0aGV1UWxYRWFSaXVrZEl0RnVjTzFhM1RrdUVabDhqQldTK3k3TEd3KzNXSzgvRXlVZncvdnpURVVNYmtON0IrcGw1SXF6Nnl0VTVDM2dVVGlsN01pWXNUeU5XQi9MbTdZU0lTREpEMllVcTBmVmV2b210Sm5Sbnc0N2d5aVErbjVDU2JCQnA5SkRsbDZtSzVlamxBbjdvWDBYaWlpcVR4Q2NvY3hwVUIyZz09LS1lMUlBcTlvU0F0K25UQ3loNHFOZExnPT0%3D--8764e6d2279a0e6960577a66864e6018ef213b56; path=/; secure; HttpOnly

2019-12-01 15:25:18 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://github.com/&gt; (referer: https://github.com/login)
['Issues', 'Issues']
2019-12-01 15:25:18 [scrapy.core.engine] INFO: Closing spider (finished)
</code></pre></div></div>

<p>æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å·²ç»æ‰“å°äº†æˆ‘ä»¬éœ€è¦çš„ä¿¡æ¯ï¼Œç™»å½•æˆåŠŸã€‚</p>

<p>Scrapy å¯¹äºè¡¨å•è¯·æ±‚ï¼Œ<code class="highlighter-rouge">FormRequest</code> è¿˜æä¾›äº†å¦å¤–ä¸€ä¸ªæ–¹æ³• <code class="highlighter-rouge">from_response</code> æ¥è‡ªåŠ¨è·å–é¡µé¢ä¸­çš„è¡¨å•ï¼Œæˆ‘ä»¬åªéœ€è¦ä¼ å…¥ç”¨æˆ·åå’Œå¯†ç å°±å¯ä»¥å‘é€è¯·æ±‚ã€‚</p>

<p>æˆ‘ä»¬æ¥çœ‹ä¸‹è¿™ä¸ªæ–¹æ³•çš„æºç ï¼š</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@classmethod
    def from_response(cls, response, formname=None, formid=None, formnumber=0, formdata=None,
                      clickdata=None, dont_click=False, formxpath=None, formcss=None, **kwargs):

        kwargs.setdefault('encoding', response.encoding)

        if formcss is not None:
            from parsel.csstranslator import HTMLTranslator
            formxpath = HTMLTranslator().css_to_xpath(formcss)

        form = _get_form(response, formname, formid, formnumber, formxpath)
        formdata = _get_inputs(form, formdata, dont_click, clickdata, response)
        url = _get_form_url(form, kwargs.pop('url', None))

        method = kwargs.pop('method', form.method)
        if method is not None:
            method = method.upper()
            if method not in cls.valid_form_methods:
                method = 'GET'

        return cls(url=url, method=method, formdata=formdata, **kwargs)
</code></pre></div></div>

<p>æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™ä¸ªæ–¹æ³•çš„å‚æ•°æœ‰å¥½å¤šï¼Œéƒ½æ˜¯æœ‰å…³ form å®šä½çš„ä¿¡æ¯ã€‚å¦‚æœç™»å½•ç½‘é¡µä¸­åªæœ‰ä¸€ä¸ªè¡¨å•ï¼Œ Scrapy å¯ä»¥å¾ˆå®¹æ˜“å®šä½ï¼Œä½†æ˜¯å¦‚æœç½‘é¡µä¸­å«æœ‰å¤šä¸ªè¡¨å•å‘¢ï¼Ÿè¿™ä¸ªæ—¶å€™æˆ‘ä»¬å°±éœ€è¦é€šè¿‡è¿™äº›å‚æ•°æ¥å‘Šè¯‰ Scrapy å“ªä¸ªæ‰æ˜¯ç™»å½•çš„è¡¨å•ã€‚</p>

<p>å½“ç„¶ï¼Œè¿™ä¸ªæ–¹æ³•çš„å‰ææ˜¯éœ€è¦æˆ‘ä»¬ç½‘é¡µçš„ form è¡¨å•çš„ action é‡Œé¢åŒ…å«äº†æäº¤è¯·æ±‚çš„ url åœ°å€ã€‚</p>

<p>åœ¨ github è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬çš„ç™»å½•é¡µé¢åªæœ‰ä¸€ä¸ªç™»å½•çš„è¡¨å•ï¼Œå› æ­¤æˆ‘ä»¬åªéœ€è¦ä¼ å…¥ç”¨æˆ·åå’Œå¯†ç å°±å¯ä»¥äº†ã€‚ä»£ç å¦‚ä¸‹ï¼š</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># -*- coding: utf-8 -*-
import scrapy
import re

class Github2Spider(scrapy.Spider):
    name = 'github2'
    allowed_domains = ['github.com']
    start_urls = ['http://github.com/login']

    def parse(self, response):
        yield scrapy.FormRequest.from_response(
            response, # è‡ªåŠ¨ä»responseä¸­å¯»æ‰¾formè¡¨å•
            formdata={"login": "xxx@qq.com", "password": "xxx"},
            callback=self.after_login
        )
    # ç™»å½•æˆåŠŸä¹‹åæ“ä½œ
    def after_login(self, response):
        # æ‰¾åˆ°é¡µé¢ä¸Šçš„ Issues å­—æ®µå¹¶æ‰“å°
        print(re.findall("Issues", response.body.decode()))
</code></pre></div></div>

<p>è¿è¡Œçˆ¬è™«åï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å’Œä¹‹å‰ä¸€æ ·çš„ç»“æœã€‚</p>

<p>è¿™ç§è¯·æ±‚æ–¹å¼æ˜¯ä¸æ˜¯ç®€å•äº†è®¸å¤šï¼Ÿä¸éœ€è¦æˆ‘ä»¬è´¹åŠ›å»æ‰¾å„ç§è¯·æ±‚å‚æ•°ï¼Œæœ‰æ²¡æœ‰è§‰å¾— Amazing ï¼Ÿ</p>

<h2 id="æ€»ç»“">æ€»ç»“</h2>

<p>æœ¬æ–‡å‘å¤§å®¶ä»‹ç»äº† Scrapy æ¨¡æ‹Ÿç™»é™†ç½‘ç«™çš„å‡ ç§æ–¹æ³•ï¼Œå¤§å®¶å¯ä»¥è‡ªå·±è¿ç”¨æ–‡ä¸­çš„æ–¹æ³•å»å®è·µä¸€ä¸‹ã€‚å½“ç„¶ï¼Œè¿™é‡Œæ²¡æœ‰æ¶‰åŠåˆ°æœ‰éªŒè¯ç çš„æƒ…å†µï¼ŒéªŒè¯ç æ˜¯ä¸€ä¸ªå¤æ‚å¹¶ä¸”éš¾åº¦å¾ˆé«˜çš„ä¸“é¢˜ï¼Œä»¥åæœ‰æ—¶é—´å†ç»™å¤§å®¶ä»‹ç»ã€‚</p>

<blockquote>
  <p>æ–‡ä¸­ç¤ºä¾‹ä»£ç ï¼š<a href="https://github.com/JustDoPython/python-100-day">python-100-days</a></p>
</blockquote>
:ET